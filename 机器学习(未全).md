# 机器学习(未全)

## 1. 定义: 

​	从数据中学习, 获得模型, 利用模型对未知数据进行分析

## 2. 解释:



## 3. 数据集构成

- 结构: 特征+目标值

- 对于一行数据称作样本
- 有些数据集可以没有目标值

## 4. 机器学习分类

### 目标值(监督学习): 

- 类别-分类问题(离散型数据)

- 类别-回归问题(连续性数据)

### 特征值:

- 无监督学习: 没有目标值

## 5. 算法分类

- ### 监督学习

   分类: k-近邻 贝叶斯 决策树 随机森林 逻辑回归 

  回归: 线性回归 岭回归

- 无监督学习

- K-means

  

## 6. 机器学习开发流程

1. 获取数据
2. 数据处理
3. 特征工程
4. 使用机器学习算法进行训练 - 模型
5. 模型评估

## 7.学习框架和资料介绍

1. 算法是核心,数据和计算是基础

## 8. 数据集

### scikit-learn工具介绍

![image-20210527110637727](images/机器学习(未全)/image-20210527110637727.png)

### sklearn的数据集

```python
#内置数据集
sklearn.datasets.load_iris()
#大数据集
sklearn.datasets.fetch_20newsgroups(data_home=None,subset=‘train’)
#subset：'train'或者'test'，'all'，可选，选择要加载的数据集。
#训练集的“训练”，测试集的“测试”，两者的“全部”
```

**sklearn数据集返回值介绍**

- load和fetch返回的数据类型datasets.base.Bunch(字典格式)
  - data：特征数据数组，是 [n_samples * n_features] 的二维 numpy.ndarray 数组
  - target：标签数组，是 n_samples 的一维 numpy.ndarray 数组
  - DESCR：数据描述
  - feature_names：特征名,新闻数据，手写数字、回归数据集没有
  - target_names：标签名

### 2.1.3 数据集的划分

机器学习一般的数据集会划分为两个部分：

- 训练数据：用于训练，**构建模型**
- 测试数据：在模型检验时使用，用于**评估模型是否有效**

划分比例：

- 训练集：70% 80% 75%
- 测试集：30% 20% 30%

**数据集划分api**

- sklearn.model_selection.train_test_split(

  arrays, *

  options)

  - x 数据集的特征值
  - y 数据集的标签值
  - test_size 测试集的大小，一般为float
  - random_state 随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。
  - return 测试集特征训练集特征值值，训练标签，测试标签(默认随机取)

```python
from sklearn.datasets import load_iris
# 获取鸢尾花数据集
iris = load_iris()
print("鸢尾花数据集的返回值：\n", iris)
# 返回值是一个继承自字典的Bench
print("鸢尾花的特征值:\n", iris["data"])
print("鸢尾花的目标值：\n", iris.target)
print("鸢尾花特征的名字：\n", iris.feature_names)
print("鸢尾花目标值的名字：\n", iris.target_names)
print("鸢尾花的描述：\n", iris.DESCR)
```

**思考：拿到的数据是否全部都用来训练一个模型？**

### 2.1.3 数据集的划分

机器学习一般的数据集会划分为两个部分：

- 训练数据：用于训练，**构建模型**
- 测试数据：在模型检验时使用，用于**评估模型是否有效**

划分比例：

- 训练集：70% 80% 75%
- 测试集：30% 20% 30%

**数据集划分api**

- sklearn.model_selection.train_test_split(arrays, *options)
  - x 数据集的特征值
  - y 数据集的标签值
  - test_size 测试集的大小，一般为float
  - random_state 随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。
  - return 测试集特征训练集特征值值，训练标签，测试标签(默认随机取)

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split


def datasets_demo():
    """
    对鸢尾花数据集的演示
    :return: None
    """
    # 1、获取鸢尾花数据集
    iris = load_iris()
    print("鸢尾花数据集的返回值：\n", iris)
    # 返回值是一个继承自字典的Bench
    print("鸢尾花的特征值:\n", iris["data"])
    print("鸢尾花的目标值：\n", iris.target)
    print("鸢尾花特征的名字：\n", iris.feature_names)
    print("鸢尾花目标值的名字：\n", iris.target_names)
    print("鸢尾花的描述：\n", iris.DESCR)

    # 2、对鸢尾花数据集进行分割
    # 训练集的特征值x_train 测试集的特征值x_test 训练集的目标值y_train 测试集的目标值y_test
    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=22)
    print("x_train:\n", x_train.shape)
    # 随机数种子
    x_train1, x_test1, y_train1, y_test1 = train_test_split(iris.data, iris.target, random_state=6)
    x_train2, x_test2, y_train2, y_test2 = train_test_split(iris.data, iris.target, random_state=6)
    print("如果随机数种子不一致：\n", x_train == x_train1)
    print("如果随机数种子一致：\n", x_train1 == x_train2)

    return None
```

## 9.特征工程

特征工程是使用**专业背景知识和技巧处理数据**，**使得特征能在机器学习算法上发挥更好的作用的过程**。

- 意义：会直接影响机器学习的效果

### 特征提取

**将任意数据（如文本或图像）转换为可用于机器学习的数字特征**

> 注：特征值化是为了计算机更好的去理解数据

- 字典特征提取(特征离散化)
- 文本特征提取
- 图像特征提取（深度学习将介绍）

**特征提取API**

```python
sklearn.feature_extraction
```

### 对字典数据进行特征值化

- sklearn.feature_extraction.DictVectorizer(sparse=True,…)
  - DictVectorizer.fit_transform(X) X:字典或者包含字典的迭代器返回值：返回sparse矩阵
  - DictVectorizer.inverse_transform(X) X:array数组或者sparse矩阵 返回值:转换之前数据格式
  - DictVectorizer.get_feature_names() 返回类别名称

我们对以下数据进行特征提取

```python
[{'city': '北京','temperature':100}
{'city': '上海','temperature':60}
{'city': '深圳','temperature':30}]
```

![image-20210528085822038](images/机器学习(未全)/image-20210528085822038.png)

```python
from sklearn.feature_extraction import DictVectorizer

def dict_demo():
    """
    对字典类型的数据进行特征抽取
    :return: None
    """
    data = [{'city': '北京','temperature':100}, {'city': '上海','temperature':60}, {'city': '深圳','temperature':30}]
    # 1、实例化一个转换器类
    transfer = DictVectorizer(sparse=False)
    # 2、调用fit_transform
    data = transfer.fit_transform(data)
    print("返回的结果:\n", data)
    # 打印特征名字
    print("特征名字：\n", transfer.get_feature_names())

    return None
```

注意观察没有加上sparse=False参数的结果(稀疏矩阵, 不表示0,提高加载效率)

```python
返回的结果:
   (0, 1)    1.0
  (0, 3)    100.0
  (1, 0)    1.0
  (1, 3)    60.0
  (2, 2)    1.0
  (2, 3)    30.0
特征名字：
 ['city=上海', 'city=北京', 'city=深圳', 'temperature']
```

这个结果并不是我们想要看到的，所以加上参数，得到想要的结果：

```python
返回的结果:
 [[   0.    1.    0.  100.]
 [   1.    0.    0.   60.]
 [   0.    0.    1.   30.]]
特征名字：
 ['city=上海', 'city=北京', 'city=深圳', 'temperature']
```

之前在学习pandas中的离散化的时候，也实现了类似的效果。

我们把这个处理数据的技巧叫做”one-hot“编码：

![image-20210528090950095](images/机器学习(未全)/image-20210528090950095.png)

转化为：

![image-20210528090958474](images/机器学习(未全)/image-20210528090958474.png)

## 	文本特征提取

**作用：对文本数据进行特征值化**

- **sklearn.feature_extraction.text.CountVectorizer(stop_words=[])**
  - 返回词频矩阵

- CountVectorizer.fit_transform(X) X:文本或者包含文本字符串的可迭代对象 返回值：返回sparse矩阵
- CountVectorizer.inverse_transform(X) X:array数组或者sparse矩阵 返回值:转换之前数据格
- CountVectorizer.get_feature_names() 返回值:单词列表

- **sklearn.feature_extraction.text.TfidfVectorizer**

### 1 应用

我们对以下数据进行特征提取

```python
["life is short,i like python",
"life is too long,i dislike python"]
```

![image-20210528094147576](images/机器学习(未全)/image-20210528094147576.png)

### 流程分析

- 实例化类CountVectorizer
- 调用fit_transform方法输入数据并转换 （注意返回格式，利用toarray()进行sparse矩阵转换array数组）

```python
from sklearn.feature_extraction.text import CountVectorizer

def text_count_demo():
    """
    对文本进行特征抽取，countvetorizer
    :return: None
    """
    data = ["life is short,i like like python", "life is too long,i dislike python"]
    # 1、实例化一个转换器类
    # transfer = CountVectorizer(sparse=False)
    transfer = CountVectorizer()
    # 2、调用fit_transform
    data = transfer.fit_transform(data)
    print("文本特征抽取的结果：\n", data.toarray())
    print("返回特征名字：\n", transfer.get_feature_names())

    return None
```

返回结果：

```python
文本特征抽取的结果：
 [[0 1 1 2 0 1 1 0]
 [1 1 1 0 1 1 0 1]]
返回特征名字：
 ['dislike', 'is', 'life', 'like', 'long', 'python', 'short', 'too']
```

### 问题:如果我们将数据替换成中文？

```python
"人生苦短，我喜欢Python" "生活太长久，我不喜欢Python"
```

那么最终得到的结果是

![image-20210528094939771](images/机器学习(未全)/image-20210528094939771.png)

为什么会得到这样的结果呢，**仔细分析之后会发现英文默认是以空格分开的。其实就达到了一个分词的效果**，所以我们要对中文进行分词处理

### 3 jieba分词处理

- jieba.cut()
  - 返回词语组成的生成器

需要安装下jieba库

```python
pip3 install jieba
```

### 案例分析

对以下三句话进行特征值化

```python
今天很残酷，明天更残酷，后天很美好，
但绝对大部分是死在明天晚上，所以每个人不要放弃今天。

我们看到的从很远星系来的光是在几百万年之前发出的，
这样当我们看到宇宙时，我们是在看它的过去。

如果只用一种方式了解某样事物，你就不会真正了解它。
了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。
```

- 分析
  - 准备句子，利用jieba.cut进行分词
  - 实例化CountVectorizer
  - 将分词结果变成字符串当作fit_transform的输入值

![image-20210528095737992](images/机器学习(未全)/image-20210528095737992.png)

```python
from sklearn.feature_extraction.text import CountVectorizer
import jieba

def cut_word(text):
    """
    对中文进行分词
    "我爱北京天安门"————>"我 爱 北京 天安门"
    :param text:
    :return: text
    """
    # 用结巴对中文字符串进行分词
    text = " ".join(list(jieba.cut(text)))

    return text

def text_chinese_count_demo2():
    """
    对中文进行特征抽取
    :return: None
    """
    data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",
            "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
            "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]
    # 将原始数据转换成分好词的形式
    text_list = []
    for sent in data:
        text_list.append(cut_word(sent))
    print(text_list)

    # 1、实例化一个转换器类
    # transfer = CountVectorizer(sparse=False)
    transfer = CountVectorizer()
    # 2、调用fit_transform
    data = transfer.fit_transform(text_list)
    print("文本特征抽取的结果：\n", data.toarray())
    print("返回特征名字：\n", transfer.get_feature_names())

    return None
```

返回结果：

```python
Building prefix dict from the default dictionary ...
Dumping model to file cache /var/folders/mz/tzf2l3sx4rgg6qpglfb035_r0000gn/T/jieba.cache
Loading model cost 1.032 seconds.
['一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。', '我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。', '如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。']
Prefix dict has been built succesfully.
文本特征抽取的结果：
 [[2 0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 1 0]
 [0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 0 1]
 [1 1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0 0]]
返回特征名字：
 ['一种', '不会', '不要', '之前', '了解', '事物', '今天', '光是在', '几百万年', '发出', '取决于', '只用', '后天', '含义', '大部分', '如何', '如果', '宇宙', '我们', '所以', '放弃', '方式', '明天', '星系', '晚上', '某样', '残酷', '每个', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '还是', '这样']
```

**但如果把这样的词语特征用于分类，会出现什么问题？**

![image-20210528104719075](images/机器学习(未全)/image-20210528104719075.png)

###  Tf-idf文本特征提取

- TF-IDF的主要思想是：如果**某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现**，则认为此词或者短语具有很好的类别区分能力，适合用来分类。

- **TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。**

  

#### 5.1 公式

- 词频（term frequency，tf）指的是某一个给定的词语在该文件中出现的频率

- 逆向文档频率（inverse document frequency，idf）是一个词语普遍重要性的度量。某一特定词语的idf，可以**由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到**

  (IDF数值大的意思是在本文出现的词频率多, 但是在别的文章出现的不多,表明此词语在本文辨识度高)

![image-20210528100018046](images/机器学习(未全)/image-20210528100018046.png)

最终得出结果可以理解为重要程度。

```
注：假如一篇文件的总词语数是100个，而词语"非常"出现了5次，那么"非常"一词在该文件中的词频就是5/100=0.05。而计算文件频率（IDF）的方法是以文件集的文件总数，除以出现"非常"一词的文件数。所以，如果"非常"一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是lg（10,000,000 / 1,0000）=3。最后"非常"对于这篇文档的tf-idf的分数为0.05 * 3=0.15
```

![image-20210528105758583](images/机器学习(未全)/image-20210528105758583.png)

#### 5.2 案例

```python
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba

def cut_word(text):
    """
    对中文进行分词
    "我爱北京天安门"————>"我 爱 北京 天安门"
    :param text:
    :return: text
    """
    # 用结巴对中文字符串进行分词
    text = " ".join(list(jieba.cut(text)))

    return text

def text_chinese_tfidf_demo():
    """
    对中文进行特征抽取
    :return: None
    """
    data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",
            "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
            "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]
    # 将原始数据转换成分好词的形式
    text_list = []
    for sent in data:
        text_list.append(cut_word(sent))
    print(text_list)

    # 1、实例化一个转换器类
    # transfer = CountVectorizer(sparse=False)
    transfer = TfidfVectorizer(stop_words=['一种', '不会', '不要'])
    # 2、调用fit_transform
    data = transfer.fit_transform(text_list)
    print("文本特征抽取的结果：\n", data.toarray())
    print("返回特征名字：\n", transfer.get_feature_names())

    return None
```

返回结果：

```python
Building prefix dict from the default dictionary ...
Loading model from cache /var/folders/mz/tzf2l3sx4rgg6qpglfb035_r0000gn/T/jieba.cache
Loading model cost 0.856 seconds.
Prefix dict has been built succesfully.
['一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。', '我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。', '如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。']
文本特征抽取的结果：
 [[ 0.          0.          0.          0.43643578  0.          0.          0.
   0.          0.          0.21821789  0.          0.21821789  0.          0.
   0.          0.          0.21821789  0.21821789  0.          0.43643578
   0.          0.21821789  0.          0.43643578  0.21821789  0.          0.
   0.          0.21821789  0.21821789  0.          0.          0.21821789
   0.        ]
 [ 0.2410822   0.          0.          0.          0.2410822   0.2410822
   0.2410822   0.          0.          0.          0.          0.          0.
   0.          0.2410822   0.55004769  0.          0.          0.          0.
   0.2410822   0.          0.          0.          0.          0.48216441
   0.          0.          0.          0.          0.          0.2410822
   0.          0.2410822 ]
 [ 0.          0.644003    0.48300225  0.          0.          0.          0.
   0.16100075  0.16100075  0.          0.16100075  0.          0.16100075
   0.16100075  0.          0.12244522  0.          0.          0.16100075
   0.          0.          0.          0.16100075  0.          0.          0.
   0.3220015   0.16100075  0.          0.          0.16100075  0.          0.
   0.        ]]
返回特征名字：
 ['之前', '了解', '事物', '今天', '光是在', '几百万年', '发出', '取决于', '只用', '后天', '含义', '大部分', '如何', '如果', '宇宙', '我们', '所以', '放弃', '方式', '明天', '星系', '晚上', '某样', '残酷', '每个', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '还是', '这样']
```

### Tf-idf的重要性

**分类机器学习算法进行文章分类中前期数据处理方式**

### 特征预处理

![image-20210528110000046](images/机器学习(未全)/image-20210528110000046.png)

## 2.4.1 什么是特征预处理

```python
# scikit-learn的解释
provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.
```

翻译过来：通过**一些转换函数**将特征数据**转换成更加适合算法模型**的特征数据过程

可以通过上面那张图来理解

### 1 包含内容

- 数值型数据的无量纲化：
  - 归一化
  - 标准化

### 2 特征预处理API

```python
sklearn.preprocessing
```

#### 为什么我们要进行归一化/标准化？

- 特征的**单位或者大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级**，**容易影响（支配）目标结果**，使得一些算法无法学习到其它的特征

### 约会对象数据

![image-20210528110158126](images/机器学习(未全)/image-20210528110158126.png)

我们需要用到一些方法进行**无量纲化**，**使不同规格的数据转换到同一规格**

## .4.2 归一化

### 1 定义

通过对原始数据进行变换把数据映射到(默认为[0,1])之间

### 2 公式

![image-20210528110442249](images/机器学习(未全)/image-20210528110442249.png)

> 作用于每一列，max为一列的最大值，min为一列的最小值,那么X’’为最终结果，mx，mi分别为指定区间值默认mx为1,mi为0

那么怎么理解这个过程呢？我们通过一个例子

![image-20210528110504862](images/机器学习(未全)/image-20210528110504862.png)

### 3 API

- sklearn.preprocessing.MinMaxScaler (feature_range=(0,1)… )
  - MinMaxScalar.fit_transform(X)
    - X:numpy array格式的数据[n_samples,n_features]
  - 返回值：转换后的形状相同的array

### 4 数据计算

我们对以下数据进行运算，在dating.txt中。保存的就是之前的约会对象数据

```python
milage,Liters,Consumtime,target
40920,8.326976,0.953952,3
14488,7.153469,1.673904,2
26052,1.441871,0.805124,1
75136,13.147394,0.428964,1
38344,1.669788,0.134296,1
```

- 分析

1、实例化MinMaxScalar

2、通过fit_transform转换

```python
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

def minmax_demo():
    """
    归一化演示
    :return: None
    """
    data = pd.read_csv("dating.txt")
    print(data)
    # 1、实例化一个转换器类
    transfer = MinMaxScaler(feature_range=(2, 3))
    # 2、调用fit_transform
    data = transfer.fit_transform(data[['milage','Liters','Consumtime']])
    print("最小值最大值归一化处理的结果：\n", data)

    return None
```

返回结果：

```python
     milage     Liters  Consumtime  target
0     40920   8.326976    0.953952       3
1     14488   7.153469    1.673904       2
2     26052   1.441871    0.805124       1
3     75136  13.147394    0.428964       1
..      ...        ...         ...     ...
998   48111   9.134528    0.728045       3
999   43757   7.882601    1.332446       3

[1000 rows x 4 columns]
最小值最大值归一化处理的结果：
 [[ 2.44832535  2.39805139  2.56233353]
 [ 2.15873259  2.34195467  2.98724416]
 [ 2.28542943  2.06892523  2.47449629]
 ..., 
 [ 2.29115949  2.50910294  2.51079493]
 [ 2.52711097  2.43665451  2.4290048 ]
 [ 2.47940793  2.3768091   2.78571804]]
```

### 问题：如果数据中异常点较多，会有什么影响？

![image-20210528111220459](images/机器学习(未全)/image-20210528111220459.png)

### 5 归一化总结

注意最大值最小值是变化的，另外，最大值与最小值非常容易受异常点影响，**所以这种方法鲁棒性较差，只适合传统精确小数据场景。**

怎么办？

## 2.4.3 标准化

### 1 定义

通过对原始数据进行变换把数据变换到均值为0,标准差为1范围内

### 2 公式

![image-20210528111412485](images/机器学习(未全)/image-20210528111412485.png)

> 作用于每一列，mean为平均值，σ为标准差

所以回到刚才异常点的地方，我们再来看看标准化

![image-20210528111527429](images/机器学习(未全)/image-20210528111527429.png)

- 对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变
- 对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。

### 3 API

- sklearn.preprocessing.StandardScaler( )
  - 处理之后每列来说所有数据都聚集在均值0附近标准差差为1
  - StandardScaler.fit_transform(X)
    - X:numpy array格式的数据[n_samples,n_features]
  - 返回值：转换后的形状相同的array

**注意：训练集的标准化需要使用fit_transfer, 但是测试集的标准化要使用transfer,  原因是训练集和测试集的样本标准差方差等不能有差异,  需要使用训练集的样本总体去预测测试集的, 故测试集需要使用训练集的样本标准差方差等数据来计算(认为测试集和训练集来自同一个样本总体)**

### 4 数据计算

同样对上面的数据进行处理

- 分析

1、实例化StandardScaler

2、通过fit_transform转换

```python
import pandas as pd
from sklearn.preprocessing import StandardScaler

def stand_demo():
    """
    标准化演示
    :return: None
    """
    data = pd.read_csv("dating.txt")
    print(data)
    # 1、实例化一个转换器类
    transfer = StandardScaler()
    # 2、调用fit_transform
    data = transfer.fit_transform(data[['milage','Liters','Consumtime']])
    print("标准化的结果:\n", data)
    print("每一列特征的平均值：\n", transfer.mean_)
    print("每一列特征的方差：\n", transfer.var_)

    return None
```

返回结果：

```python
     milage     Liters  Consumtime  target
0     40920   8.326976    0.953952       3
1     14488   7.153469    1.673904       2
2     26052   1.441871    0.805124       1
..      ...        ...         ...     ...
997   26575  10.650102    0.866627       3
998   48111   9.134528    0.728045       3
999   43757   7.882601    1.332446       3

[1000 rows x 4 columns]
标准化的结果:
 [[ 0.33193158  0.41660188  0.24523407]
 [-0.87247784  0.13992897  1.69385734]
 [-0.34554872 -1.20667094 -0.05422437]
 ..., 
 [-0.32171752  0.96431572  0.06952649]
 [ 0.65959911  0.60699509 -0.20931587]
 [ 0.46120328  0.31183342  1.00680598]]
每一列特征的平均值：
 [  3.36354210e+04   6.55996083e+00   8.32072997e-01]
每一列特征的方差：
 [  4.81628039e+08   1.79902874e+01   2.46999554e-01]
```

### 5 标准化总结

在已有样本足够多的情况下比较稳定，适合**现代嘈杂大数据场景。**

## 2.5 特征降维

**降维**是指在某些限定条件下，**降低随机变量(特征)个数**，得到**一组“不相关”主变量**的过程

- 降低随机变量的个数

![image-20210528112126023](images/机器学习(未全)/image-20210528112126023.png)

- 相关特征(correlated feature)
  - 相对湿度与降雨量之间的相关
  - 等等

> 正是因为在进行训练的时候，我们都是使用特征进行学习。如果特征本身存在问题或者特征之间相关性较强，对于算法学习预测会影响较大

## 2.5.2 降维的两种方式

- **特征选择**
- **主成分分析（可以理解一种特征提取的方式）**

## 2.5.3 什么是特征选择

### 1 定义

数据中包含**冗余或无关变量（或称特征、属性、指标等）**，旨在从**原有特征中找出主要特征**。

![image-20210528112255870](images/机器学习(未全)/image-20210528112255870.png)

### 2 方法

- Filter(过滤式)：主要探究特征本身特点、特征与特征和目标值之间关联
  - **方差选择法：低方差特征过滤**
  - **相关系数**
- Embedded (嵌入式)：算法自动选择特征（特征与目标值之间的关联）
  - **决策树:信息熵、信息增益**
  - **正则化：L1、L2**
  - **深度学习：卷积等**

> 对于Embedded方式，只能在讲解算法的时候在进行介绍，更好的去理解

### 3 模块

```python
sklearn.feature_selection
```

### 4 过滤式

#### 4.1 低方差特征过滤

删除低方差的一些特征，前面讲过方差的意义。再结合方差的大小来考虑这个方式的角度。

- 特征方差小：某个特征大多样本的值比较相近
- 特征方差大：某个特征很多样本的值都有差别

##### 4.1.1 API

- sklearn.feature_selection.VarianceThreshold(threshold = 0.0)
  - 删除所有低方差特征
  - Variance.fit_transform(X)
    - X:numpy array格式的数据[n_samples,n_features]
    - 返回值：训练集差异低于threshold的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。

##### 4.1.2 数据计算

我们对**某些股票的指标特征之间进行一个筛选**，数据在"factor_regression_data/factor_returns.csv"文件当中,除去'index,'date','return'列不考虑**（这些类型不匹配，也不是所需要指标）**

一共这些特征

```
pe_ratio,pb_ratio,market_cap,return_on_asset_net_profit,du_return_on_equity,ev,earnings_per_share,revenue,total_expense
index,pe_ratio,pb_ratio,market_cap,return_on_asset_net_profit,du_return_on_equity,ev,earnings_per_share,revenue,total_expense,date,return
0,000001.XSHE,5.9572,1.1818,85252550922.0,0.8008,14.9403,1211444855670.0,2.01,20701401000.0,10882540000.0,2012-01-31,0.027657228229937388
1,000002.XSHE,7.0289,1.588,84113358168.0,1.6463,7.8656,300252061695.0,0.326,29308369223.2,23783476901.2,2012-01-31,0.08235182370820669
2,000008.XSHE,-262.7461,7.0003,517045520.0,-0.5678,-0.5943,770517752.56,-0.006,11679829.03,12030080.04,2012-01-31,0.09978900335112327
3,000060.XSHE,16.476,3.7146,19680455995.0,5.6036,14.617,28009159184.6,0.35,9189386877.65,7935542726.05,2012-01-31,0.12159482758620697
4,000069.XSHE,12.5878,2.5616,41727214853.0,2.8729,10.9097,81247380359.0,0.271,8951453490.28,7091397989.13,2012-01-31,-0.0026808154146886697
```

- 分析

1、初始化VarianceThreshold,指定阀值方差

2、调用fit_transform

```python
def variance_demo():
    """
    删除低方差特征——特征选择
    :return: None
    """
    data = pd.read_csv("factor_returns.csv")
    print(data)
    # 1、实例化一个转换器类
    transfer = VarianceThreshold(threshold=1)
    # 2、调用fit_transform
    data = transfer.fit_transform(data.iloc[:, 1:10])
    print("删除低方差特征的结果：\n", data)
    print("形状：\n", data.shape)

    return None
```

返回结果：

```python
            index  pe_ratio  pb_ratio    market_cap  \
0     000001.XSHE    5.9572    1.1818  8.525255e+10   
1     000002.XSHE    7.0289    1.5880  8.411336e+10    
...           ...       ...       ...           ...   
2316  601958.XSHG   52.5408    2.4646  3.287910e+10   
2317  601989.XSHG   14.2203    1.4103  5.911086e+10   

      return_on_asset_net_profit  du_return_on_equity            ev  \
0                         0.8008              14.9403  1.211445e+12   
1                         1.6463               7.8656  3.002521e+11    
...                          ...                  ...           ...   
2316                      2.7444               2.9202  3.883803e+10   
2317                      2.0383               8.6179  2.020661e+11   

      earnings_per_share       revenue  total_expense        date    return  
0                 2.0100  2.070140e+10   1.088254e+10  2012-01-31  0.027657  
1                 0.3260  2.930837e+10   2.378348e+10  2012-01-31  0.082352  
2                -0.0060  1.167983e+07   1.203008e+07  2012-01-31  0.099789   
...                  ...           ...            ...         ...       ...  
2315              0.2200  1.789082e+10   1.749295e+10  2012-11-30  0.137134  
2316              0.1210  6.465392e+09   6.009007e+09  2012-11-30  0.149167  
2317              0.2470  4.509872e+10   4.132842e+10  2012-11-30  0.183629  

[2318 rows x 12 columns]
删除低方差特征的结果：
 [[  5.95720000e+00   1.18180000e+00   8.52525509e+10 ...,   1.21144486e+12
    2.07014010e+10   1.08825400e+10]
 [  7.02890000e+00   1.58800000e+00   8.41133582e+10 ...,   3.00252062e+11
    2.93083692e+10   2.37834769e+10]
 [ -2.62746100e+02   7.00030000e+00   5.17045520e+08 ...,   7.70517753e+08
    1.16798290e+07   1.20300800e+07]
 ..., 
 [  3.95523000e+01   4.00520000e+00   1.70243430e+10 ...,   2.42081699e+10
    1.78908166e+10   1.74929478e+10]
 [  5.25408000e+01   2.46460000e+00   3.28790988e+10 ...,   3.88380258e+10
    6.46539204e+09   6.00900728e+09]
 [  1.42203000e+01   1.41030000e+00   5.91108572e+10 ...,   2.02066110e+11
    4.50987171e+10   4.13284212e+10]]
形状：
 (2318, 8)
```

#### 4.2 相关系数

- 皮尔逊相关系数(Pearson Correlation Coefficient)
  - 反映变量之间相关关系密切程度的统计指标

##### 4.2.2 公式计算案例(了解，不用记忆)

- 公式

![image-20210528130400578](images/机器学习(未全)/image-20210528130400578.png)

- 比如说我们计算年广告费投入与月均销售额

![image-20210528130414856](images/机器学习(未全)/image-20210528130414856.png)

那么之间的相关系数怎么计算

![image-20210528130429495](images/机器学习(未全)/image-20210528130429495.png)

最终计算：

![image-20210528130440422](images/机器学习(未全)/image-20210528130440422.png)

= 0.9942

**所以我们最终得出结论是广告投入费与月平均销售额之间有高度的正相关关系。** 　

##### 4.2.3 特点

**相关系数的值介于–1与+1之间，即–1≤ r ≤+1**。其性质如下：

- **当r>0时，表示两变量正相关，r<0时，两变量为负相关**
- 当|r|=1时，表示两变量为完全相关，当r=0时，表示两变量间无相关关系
- **当0<|r|<1时，表示两变量存在一定程度的相关。且|r|越接近1，两变量间线性关系越密切；|r|越接近于0，表示两变量的线性相关越弱**
- **一般可按三级划分：|r|<0.4为低度相关；0.4≤|r|<0.7为显著性相关；0.7≤|r|<1为高度线性相关**

> 这个符号：|r|为r的绝对值， |-5| = 5

##### 4.2.4 API

- from scipy.stats import pearsonr
  - x : (N,) array_like
  - y : (N,) array_like Returns: (Pearson’s correlation coefficient, p-value)

##### 4.2.5 案例：股票的财务指标相关性计算

我们刚才的股票的这些指标进行相关性计算， 假设我们以

```python
factor = ['pe_ratio','pb_ratio','market_cap','return_on_asset_net_profit','du_return_on_equity','ev','earnings_per_share','revenue','total_expense']
```

这些特征当中的两两进行计算，得出相关性高的一些特征

![image-20210528130802808](images/机器学习(未全)/image-20210528130802808.png)

- 分析
  - 两两特征之间进行相关性计算

```python
import pandas as pd
from scipy.stats import pearsonr

def pearsonr_demo():
    """
    相关系数计算
    :return: None
    """
    data = pd.read_csv("factor_returns.csv")

    factor = ['pe_ratio', 'pb_ratio', 'market_cap', 'return_on_asset_net_profit', 'du_return_on_equity', 'ev',
              'earnings_per_share', 'revenue', 'total_expense']

    for i in range(len(factor)):
        for j in range(i, len(factor) - 1):
            print(
                "指标%s与指标%s之间的相关性大小为%f" % (factor[i], factor[j + 1], pearsonr(data[factor[i]], data[factor[j + 1]])[0]))

    return None
```

返回结果：

```python
指标pe_ratio与指标pb_ratio之间的相关性大小为-0.004389
指标pe_ratio与指标market_cap之间的相关性大小为-0.068861
指标pe_ratio与指标return_on_asset_net_profit之间的相关性大小为-0.066009
指标pe_ratio与指标du_return_on_equity之间的相关性大小为-0.082364
指标pe_ratio与指标ev之间的相关性大小为-0.046159
指标pe_ratio与指标earnings_per_share之间的相关性大小为-0.072082
指标pe_ratio与指标revenue之间的相关性大小为-0.058693
指标pe_ratio与指标total_expense之间的相关性大小为-0.055551
指标pb_ratio与指标market_cap之间的相关性大小为0.009336
指标pb_ratio与指标return_on_asset_net_profit之间的相关性大小为0.445381
指标pb_ratio与指标du_return_on_equity之间的相关性大小为0.291367
指标pb_ratio与指标ev之间的相关性大小为-0.183232
指标pb_ratio与指标earnings_per_share之间的相关性大小为0.198708
指标pb_ratio与指标revenue之间的相关性大小为-0.177671
指标pb_ratio与指标total_expense之间的相关性大小为-0.173339
指标market_cap与指标return_on_asset_net_profit之间的相关性大小为0.214774
指标market_cap与指标du_return_on_equity之间的相关性大小为0.316288
指标market_cap与指标ev之间的相关性大小为0.565533
指标market_cap与指标earnings_per_share之间的相关性大小为0.524179
指标market_cap与指标revenue之间的相关性大小为0.440653
指标market_cap与指标total_expense之间的相关性大小为0.386550
指标return_on_asset_net_profit与指标du_return_on_equity之间的相关性大小为0.818697
指标return_on_asset_net_profit与指标ev之间的相关性大小为-0.101225
指标return_on_asset_net_profit与指标earnings_per_share之间的相关性大小为0.635933
指标return_on_asset_net_profit与指标revenue之间的相关性大小为0.038582
指标return_on_asset_net_profit与指标total_expense之间的相关性大小为0.027014
指标du_return_on_equity与指标ev之间的相关性大小为0.118807
指标du_return_on_equity与指标earnings_per_share之间的相关性大小为0.651996
指标du_return_on_equity与指标revenue之间的相关性大小为0.163214
指标du_return_on_equity与指标total_expense之间的相关性大小为0.135412
指标ev与指标earnings_per_share之间的相关性大小为0.196033
指标ev与指标revenue之间的相关性大小为0.224363
指标ev与指标total_expense之间的相关性大小为0.149857
指标earnings_per_share与指标revenue之间的相关性大小为0.141473
指标earnings_per_share与指标total_expense之间的相关性大小为0.105022
指标revenue与指标total_expense之间的相关性大小为0.995845
```

从中我们得出

- 指标revenue与指标total_expense之间的相关性大小为0.995845
- 指标return_on_asset_net_profit与指标du_return_on_equity之间的相关性大小为0.818697

我们也可以通过画图来观察结果

```python
import matplotlib.pyplot as plt
plt.figure(figsize=(20, 8), dpi=100)
plt.scatter(data['revenue'], data['total_expense'])
plt.show()
```

![image-20210528132323509](images/机器学习(未全)/image-20210528132323509.png)

**这两对指标之间的相关性较大，可以做之后的处理，比如合成这两个指标。**

## 2.6主成分分析

- 定义：**高维数据转化为低维数据的 过程**，在此过程中**可能会舍弃原有数据、创造新的变量**
- 作用：**是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。**
- 应用：回归分析或者聚类分析当中

> 对于信息一词，在决策树中会进行介绍

那么更好的理解这个过程呢？我们来看一张图

![image-20210528132632093](images/机器学习(未全)/image-20210528132632093.png)

### 1 计算案例理解(了解，无需记忆)

假设对于给定5个点，数据如下

```
(-1,-2)
(-1, 0)
( 0, 0)
( 2, 1)
( 0, 1)
```

![image-20210528132852661](images/机器学习(未全)/image-20210528132852661.png)

要求：将这个二维的数据简化成一维？ 并且损失少量的信息

![image-20210528132913225](images/机器学习(未全)/image-20210528132913225.png)

这个过程如何计算的呢？**找到一个合适的直线，通过一个矩阵运算得出主成分分析的结果（不需要理解）**

![image-20210528132930647](images/机器学习(未全)/image-20210528132930647.png)

### 2 API

- sklearn.decomposition.PCA(n_components=None)
  - 将数据分解为较低维数空间
  - n_components:
    - **小数：表示保留百分之多少的信息**
    - **整数：减少到多少特征**
  - PCA.fit_transform(X) X:numpy array格式的数据[n_samples,n_features]
  - 返回值：转换后指定维度的array

### 3 数据计算

先拿个简单的数据计算一下

```python
[[2,8,4,5],
[6,3,0,8],
[5,4,9,1]]
from sklearn.decomposition import PCA

def pca_demo():
    """
    对数据进行PCA降维
    :return: None
    """
    data = [[2,8,4,5], [6,3,0,8], [5,4,9,1]]

    # 1、实例化PCA, 小数——保留多少信息
    transfer = PCA(n_components=0.9)
    # 2、调用fit_transform
    data1 = transfer.fit_transform(data)

    print("保留90%的信息，降维结果为：\n", data1)

    # 1、实例化PCA, 整数——指定降维到的维数
    transfer2 = PCA(n_components=3)
    # 2、调用fit_transform
    data2 = transfer2.fit_transform(data)
    print("降维到3维的结果：\n", data2)

    return None
```

返回结果：

```python
保留90%的信息，降维结果为：
 [[ -3.13587302e-16   3.82970843e+00]
 [ -5.74456265e+00  -1.91485422e+00]
 [  5.74456265e+00  -1.91485422e+00]]
降维到3维的结果：
 [[ -3.13587302e-16   3.82970843e+00   4.59544715e-16]
 [ -5.74456265e+00  -1.91485422e+00   4.59544715e-16]
 [  5.74456265e+00  -1.91485422e+00   4.59544715e-16]]
```

## 2.6.2 案例：探究用户对物品类别的喜好细分降维

![image-20210528134317342](images/机器学习(未全)/image-20210528134317342.png)

数据如下：

- order_products__prior.csv：订单与商品信息
  - 字段：**order_id**, **product_id**, add_to_cart_order, reordered
- products.csv：商品信息
  - 字段：**product_id**, product_name, **aisle_id**, department_id
- orders.csv：用户的订单信息
  - 字段：**order_id**,**user_id**,eval_set,order_number,….
- aisles.csv：商品所属具体物品类别
  - 字段： **aisle_id**, **aisle**

### 1 需求

![image-20210528134338737](images/机器学习(未全)/image-20210528134338737.png)

### 2 分析

- 合并表，使得**user_id**与**aisle**在一张表当中
- 进行交叉表变换
- 进行降维

### 3 完整代码

```python
import pandas as pd
from sklearn.decomposition import PCA

# 1、获取数据集
# ·商品信息- products.csv：
# Fields：product_id, product_name, aisle_id, department_id
# ·订单与商品信息- order_products__prior.csv：
# Fields：order_id, product_id, add_to_cart_order, reordered 
# ·用户的订单信息- orders.csv：
# Fields：order_id, user_id,eval_set, order_number,order_dow, order_hour_of_day, days_since_prior_order 
# ·商品所属具体物品类别- aisles.csv：
# Fields：aisle_id, aisle     
products = pd.read_csv("./instacart/products.csv")
order_products = pd.read_csv("./instacart/order_products__prior.csv")
orders = pd.read_csv("./instacart/orders.csv")
aisles = pd.read_csv("./instacart/aisles.csv")

# 2、合并表，将user_id和aisle放在一张表上
# 1）合并orders和order_products on=order_id tab1:order_id, product_id, user_id
tab1 = pd.merge(orders, order_products, on=["order_id", "order_id"])
# 2）合并tab1和products on=product_id tab2:aisle_id
tab2 = pd.merge(tab1, products, on=["product_id", "product_id"])
# 3）合并tab2和aisles on=aisle_id tab3:user_id, aisle
tab3 = pd.merge(tab2, aisles, on=["aisle_id", "aisle_id"])

# 3、交叉表处理，把user_id和aisle进行分组
table = pd.crosstab(tab3["user_id"], tab3["aisle"])

# 4、主成分分析的方法进行降维
# 1）实例化一个转换器类PCA
transfer = PCA(n_components=0.95)
# 2）fit_transform
data = transfer.fit_transform(table)

data.shape
```

返回结果：

```python
(206209, 44)
```

## 3.0分类算法

## 1、转换器和估计器

### 1.1 转换器

想一下之前做的特征工程的步骤？

- 1、实例化 (实例化的是一个转换器类(Transformer))
- 2、调用fit_transform(对于文档建立分类词频矩阵，不能同时调用)

我们把特征工程的接口称之为转换器，其中转换器调用有这么几种形式

- fit_transform
- fit    
- transform       

**这几个方法之间的区别是什么呢？我们看以下代码就清楚了**

```python
In [1]: from sklearn.preprocessing import StandardScaler

In [2]: std1 = StandardScaler()

In [3]: a = [[1,2,3], [4,5,6]]

In [4]: std1.fit_transform(a)
Out[4]:
array([[-1., -1., -1.],
       [ 1.,  1.,  1.]])

In [5]: std2 = StandardScaler()

In [6]: std2.fit(a)
Out[6]: StandardScaler(copy=True, with_mean=True, with_std=True)

In [7]: std2.transform(a)
Out[7]:
array([[-1., -1., -1.],
       [ 1.,  1.,  1.]])
```

从中可以看出，fit_transform的作用相当于transform加上fit。但是为什么还要提供单独的fit呢, 我们还是使用原来的std2来进行标准化看看

```python
In [8]: b = [[7,8,9], [10, 11, 12]]

In [9]: std2.transform(b)
Out[9]:
array([[3., 3., 3.],
       [5., 5., 5.]])

In [10]: std2.fit_transform(b)
Out[10]:
array([[-1., -1., -1.],
       [ 1.,  1.,  1.]])
```

### 1.2 估计器(sklearn机器学习算法的实现)

在sklearn中，估计器(estimator)是一个重要的角色，是一类实现了算法的API

- 1、用于分类的估计器：
  - sklearn.neighbors k-近邻算法
  - sklearn.naive_bayes 贝叶斯
  - sklearn.linear_model.LogisticRegression 逻辑回归
  - sklearn.tree 决策树与随机森林
- 2、用于回归的估计器：
  - sklearn.linear_model.LinearRegression 线性回归
  - sklearn.linear_model.Ridge 岭回归
- 3、用于无监督学习的估计器
  - sklearn.cluster.KMeans 聚类

### 1.3 估计器工作流程

![image-20210528142319171](images/机器学习(未全)/image-20210528142319171.png)

## 3.2 KNN算法

## 什么是K-近邻算法

![image-20210528142725191](images/机器学习(未全)/image-20210528142725191.png)

- 你的“邻居”来推断出你的类别

## 1、K-近邻算法(KNN)

### 1.1 定义

如果一个样本在特征空间中的**k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别**，则该样本也属于这个类别。

> 来源：KNN算法最早是由Cover和Hart提出的一种分类算法

### 1.2 距离公式

两个样本的距离可以通过如下公式计算，又叫欧式距离

![image-20210528143106496](images/机器学习(未全)/image-20210528143106496.png)

## 2、电影类型分析

假设我们有现在几部电影

![image-20210528143506198](images/机器学习(未全)/image-20210528143506198.png)

其中？ 号电影不知道类别，如何去预测？我们可以利用K近邻算法的思想

![image-20210528143545234](images/机器学习(未全)/image-20210528143545234.png)

### 2.1 问题

- 如果取的最近的电影数量不一样？会是什么结果？

  (在k个最近的点里, 看哪个类多就分到哪个类)

  k过小, 容易受到异常值影响(取1, 只针对最近的点)

  k过大, 易受到样本不均衡影响(取全, 谁多就是谁了)

### 2.2 K-近邻算法数据的特征工程处理

- 结合前面的约会对象数据，分析K-近邻算法需要做什么样的处理

  做无量纲化的处理:

  ​	标准化

## 3、K-近邻算法API

- sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')
  - n_neighbors：int,可选（默认= 5），k_neighbors查询默认使用的邻居数
  - algorithm：{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选用于计算最近邻居的算法：‘ball_tree’将会使用 BallTree，‘kd_tree’将使用 KDTree。‘auto’将尝试根据传递给fit方法的值来决定最合适的算法。 (不同实现方式影响效率)

## 4、案例：预测签到位置

![image-20210528143730465](images/机器学习(未全)/image-20210528143730465.png)

数据介绍：将根据用户的位置，准确性和时间戳预测用户正在查看的业务。

```python
train.csv，test.csv 
row_id：登记事件的ID
xy：坐标
准确性：定位准确性 
时间：时间戳
place_id：业务的ID，这是您预测的目标
```

> 官网：https://www.kaggle.com/navoshta/grid-knn/data

### 4.1 分析

- 对于数据做一些基本处理（这里所做的一些处理不一定达到很好的效果，我们只是简单尝试，有些特征我们可以根据一些特征选择的方式去做处理）

  - 1、缩小数据集范围 DataFrame.query()

  - 4、删除没用的日期数据 DataFrame.drop（可以选择保留）

  - 5、将签到位置少于n个用户的删除

    place_count = data.groupby('place_id').count()

    tf = place_count[place_count.row_id > 3].reset_index()

    data = data[data['place_id'].isin(tf.place_id)]

- 分割数据集

- 标准化处理

- k-近邻预测

### 4.2 代码

```python
def knncls():
    """
    K近邻算法预测入住位置类别
    :return:
    """
    # 一、处理数据以及特征工程
    # 1、读取收，缩小数据的范围
    data = pd.read_csv("./data/FBlocation/train.csv")

    # 数据逻辑筛选操作 df.query()
    data = data.query("x > 1.0 & x < 1.25 & y > 2.5 & y < 2.75")

    # 删除time这一列特征
    data = data.drop(['time'], axis=1)

    print(data)

    # 删除入住次数少于三次位置
    place_count = data.groupby('place_id').count()

    tf = place_count[place_count.row_id > 3].reset_index()

    data = data[data['place_id'].isin(tf.place_id)]

    # 3、取出特征值和目标值
    y = data['place_id']
    # y = data[['place_id']]

    x = data.drop(['place_id', 'row_id'], axis=1)

    # 4、数据分割与特征工程?

    # （1）、数据分割
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)

    # (2)、标准化
    std = StandardScaler()

    # 队训练集进行标准化操作
    x_train = std.fit_transform(x_train)
    print(x_train)

    # 进行测试集的标准化操作
    # 用训练集合的标准差方差转测试集?(少了fit)
    # x_test = std.fit_transform(x_test)
    x_test = std.transform(x_test)

    # 二、算法的输入训练预测
    # K值：算法传入参数不定的值    理论上：k = 根号(样本数)
    # K值：后面会使用参数调优方法，去轮流试出最好的参数[1,3,5,10,20,100,200]
    knn = KNeighborsClassifier(n_neighbors=1)

    # 调用fit()
    knn.fit(x_train, y_train)

    # 预测测试数据集，得出准确率
    y_predict = knn.predict(x_test)

    print("预测测试集类别：", y_predict)

    print("准确率为：", knn.score(x_test, y_test))

    return None
```

### 4.3 结果分析

### 准确率： 分类算法的评估之一

- 1、k值取多大？有什么影响？

k值取很小：容易受到异常点的影响

k值取很大：受到样本均衡的问题

- 2、性能问题？

距离计算上面，时间复杂度高

## 5、K-近邻总结

- 优点：
  - 简单，易于理解，易于实现，无需训练
- 缺点：
  - 懒惰算法，对测试样本分类时的计算量大，内存开销大
  - 必须指定K值，K值选择不当则分类精度不能保证
- 使用场景：小数据场景，几千～几万样本，具体场景具体业务去测试

# 模型选择与调优

## 学习目标

- 目标
  - 说明交叉验证过程
  - 说明超参数搜索过程
  - 应用GridSearchCV实现算法参数的调优
- 应用
  - Facebook签到位置预测调优

## 1、为什么需要交叉验证

交叉验证目的：**为了让被评估的模型更加准确可信**

## 2、什么是交叉验证(cross validation)

交叉验证：将拿到的训练数据，分为训练和验证集。以下图为例：将数据分成5份，其中一份作为验证集。然后经过5次(组)的测试，每次都更换不同的验证集。即得到5组模型的结果，取平均值作为最终结果。又称5折交叉验证。

### 2.1 分析

我们之前知道数据分为训练集和测试集，但是**为了让从训练得到模型结果更加准确。**做以下处理

- 训练集：训练集+验证集
- 测试集：测试集

![image-20210528161642303](images/机器学习(未全)/image-20210528161642303.png)

### 问题：那么这个只是对于参数得出更好的结果，那么怎么选择或者调优参数呢？

### 3、超参数搜索-网格搜索(Grid Search)

通常情况下，**有很多参数是需要手动指定的（如k-近邻算法中的K值），这种叫超参数**。但是手动过程繁杂，所以需要对模型预设几种超参数组合。**每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。**

![image-20210528161744940](images/机器学习(未全)/image-20210528161744940.png)

### 3.1 模型选择与调优

- sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None)
  - 对估计器的指定参数值进行详尽搜索
  - estimator：估计器对象
  - param_grid：估计器参数(dict){“n_neighbors”:[1,3,5]}
  - cv：指定几折交叉验证
  - fit：输入训练数据
  - score：准确率
  - 结果分析：
    - best*score*:在交叉验证中验证的最好结果_
    - best*estimator*：最好的参数模型
    - cv*results*:每次交叉验证后的验证集准确率结果和训练集准确率结果

## 4、Facebook签到位置预测K值调优

- 使用网格搜索估计器

```python
# 使用网格搜索和交叉验证找到合适的参数
knn = KNeighborsClassifier()

param = {"n_neighbors": [3, 5, 10]}

gc = GridSearchCV(knn, param_grid=param, cv=2)

gc.fit(x_train, y_train)

print("选择了某个模型测试集当中预测的准确率为：", gc.score(x_test, y_test))

# 训练验证集的结果
print("在交叉验证当中验证的最好结果：", gc.best_score_)
print("gc选择了的模型K值是：", gc.best_estimator_)
print("每次交叉验证的结果为：", gc.cv_results_)
```

# 朴素贝叶斯算法

## 学习目标

- 目标
  - 说明条件概率与联合概率
  - 说明贝叶斯公式、以及特征独立的关系
  - 记忆贝叶斯公式
  - 知道拉普拉斯平滑系数
  - 应用贝叶斯公式实现概率的计算
- 应用
  - 20类新闻文章分类预测

## 1、 什么是朴素贝叶斯分类方法



![image-20210530230852338](images/机器学习(未全)/image-20210530230852338.png)

![image-20210530230837368](images/机器学习(未全)/image-20210530230837368.png)

## 2、 概率基础

### 2.1 概率(Probability)定义

- 概率定义为一件事情发生的可能性
  - 扔出一个硬币，结果头像朝上
  - 某天是晴天
- P(X) : 取值在[0, 1]

### 2.2 女神是否喜欢计算案例

在讲这两个概率之前我们通过一个例子，来计算一些结果：

![image-20210530231134902](images/机器学习(未全)/image-20210530231134902.png)

- 问题如下：

![image-20210530231202067](images/机器学习(未全)/image-20210530231202067.png)

![image-20210530232009089](images/机器学习(未全)/image-20210530232009089.png)

那么其中有些问题我们计算的结果不正确，或者不知道计算，我们有固定的公式去计算

### 2.3 条件概率与联合概率

- 联合概率：包含多个条件，且所有条件同时成立的概率
  - 记作：P(A,B)
  - 特性：P(A, B) = P(A)P(B)
- 条件概率：就是事件A在另外一个事件B已经发生条件下的发生概率
  - 记作：P(A|B)
  - 特性：P(A1,A2|B) = P(A1|B)P(A2|B)

> 注意：此条件概率的成立，**是由于A1,A2相互独立的结果**(记忆)

这样我们计算结果为：

```python
p(程序员, 匀称) =  P(程序员)P(匀称) =3/7*(4/7) = 12/49 
P(产品, 超重|喜欢) = P(产品|喜欢)P(超重|喜欢)=1/2 *  1/4 = 1/8
```

**那么，我们知道了这些知识之后，继续回到我们的主题中。朴素贝叶斯如何分类，这个算法经常会用在文本分类，那就来看文章分类是一个什么样的问题？**

![image-20210530231444591](images/机器学习(未全)/image-20210530231444591.png)

这个了类似一个条件概率，那么仔细一想，给定文章其实相当于给定什么？结合前面我们将文本特征抽取的时候讲的？所以我们可以理解为

![image-20210530231454578](images/机器学习(未全)/image-20210530231454578.png)

**但是这个公式怎么求？前面并没有参考例子，其实是相似的，我们可以使用贝叶斯公式去计算**

## 3、 贝叶斯公式

### 3.1 公式

![image-20210530231507540](images/机器学习(未全)/image-20210530231507540.png)

**那么这个公式如果应用在文章分类的场景当中，我们可以这样看：**

![image-20210530231522894](images/机器学习(未全)/image-20210530231522894.png)

公式分为三个部分：

- P(C)：每个文档类别的概率(某文档类别数／总文档数量)
- P(W│C)：给定类别下特征（被预测文档中出现的词）的概率
  - 计算方法：P(F1│C)=Ni/N （训练文档中去计算）
    - Ni为该F1词在C类别所有文档中出现的次数
    - N为所属类别C下的文档所有词出现的次数和
- P(F1,F2,…) 预测文档中每个词的概率

如果计算两个类别概率比较：

![image-20210530232609631](images/机器学习(未全)/image-20210530232609631.png)

**所以我们只要比较前面的大小就可以，得出谁的概率大**



**什么是朴素贝叶斯=朴素+贝叶斯(假设特征和特征之间是相互独立的, 再应用贝叶斯公式)**

### 3.2 文章分类计算

- 假设我们从**训练数据集**得到如下信息

![image-20210531092629323](images/机器学习(未全)/image-20210531092629323.png)

- 计算结果

```python
科技：P(科技|影院,支付宝,云计算) = 𝑃(影院,支付宝,云计算|科技)∗P(科技)=(8/100)∗(20/100)∗(63/100)∗(30/90) = 0.00456109

娱乐：P(娱乐|影院,支付宝,云计算) = 𝑃(影院,支付宝,云计算|娱乐)∗P(娱乐)=(56/121)∗(15/121)∗(0/121)∗(60/90) = 0
```

### 思考:我们计算出来某个概率为0，合适吗？

### 3.3 拉普拉斯平滑系数

目的：防止计算出的分类概率为0

![image-20210531092645874](images/机器学习(未全)/image-20210531092645874.png)

```python
P(娱乐|影院,支付宝,云计算) =P(影院,支付宝,云计算|娱乐)P(娱乐) =P(影院|娱乐)*P(支付宝|娱乐)*P(云计算|娱乐)P(娱乐)=(56+1/121+4)(15+1/121+4)(0+1/121+1*4)(60/90) = 0.00002
```

### 3.4 API

- sklearn.naive_bayes.MultinomialNB(alpha = 1.0)
  - 朴素贝叶斯分类
  - alpha：拉普拉斯平滑系数

## 4、案例：20类新闻分类

![image-20210531092659574](images/机器学习(未全)/image-20210531092659574.png)

### 4.1 分析

- 分割数据集
- tfidf进行的特征抽取
- 朴素贝叶斯预测

### 4.2 代码

```python
def nbcls():
    """
    朴素贝叶斯对新闻数据集进行预测
    :return:
    """
    # 获取新闻的数据，20个类别
    news = fetch_20newsgroups(subset='all')

    # 进行数据集分割
    x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.3)

    # 对于文本数据，进行特征抽取
    tf = TfidfVectorizer()

    x_train = tf.fit_transform(x_train)
    # 这里打印出来的列表是：训练集当中的所有不同词的组成的一个列表
    print(tf.get_feature_names())
    # print(x_train.toarray())

    # 不能调用fit_transform
    x_test = tf.transform(x_test)

    # estimator估计器流程
    mlb = MultinomialNB(alpha=1.0)

    mlb.fit(x_train, y_train)

    # 进行预测
    y_predict = mlb.predict(x_test)

    print("预测每篇文章的类别：", y_predict[:100])
    print("真实类别为：", y_test[:100])

    print("预测准确率为：", mlb.score(x_test, y_test))

    return None
```

## 5、总结

- 优点：
  - 朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。
  - 对缺失数据不太敏感，算法也比较简单，常用于文本分类。
  - 分类准确度高，速度快
- 缺点：
  - 由于使用了样本属性独立性的假设，所以如果特征属性有关联时其效果不好

# 决策树

## 学习目标

- 目标
  - 说明信息熵的公式以及作用
  - 说明信息增益的公式作用
  -  应用信息增益实现计算特征的不确定性减少程度
  - 了解决策树的三种算法实现
- 应用
  - 泰坦尼克号乘客生存预测

## 1、认识决策树

决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-then结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法

怎么理解这句话？通过一个对话例子

![image-20210531100112728](images/机器学习(未全)/image-20210531100112728.png)

想一想这个女生为什么把年龄放在最上面判断！！！！！！！！！

## 2、决策树分类原理详解

为了更好理解决策树具体怎么分类的，我们通过一个问题例子？

![image-20210531100225855](images/机器学习(未全)/image-20210531100225855.png)

#### 问题：如何对这些客户进行分类预测？你是如何去划分？

有可能你的划分是这样的

![image-20210531100247966](images/机器学习(未全)/image-20210531100247966.png)

那么我们怎么知道这些特征哪个更好放在最上面，那么决策树的真是划分是这样的

![image-20210531100304358](images/机器学习(未全)/image-20210531100304358.png)

## 2.1 原理

- 信息熵、信息增益等

**需要用到信息论的知识！！！问题：通过例子引入信息熵**

### 2.2 信息熵

那来玩个猜测游戏，猜猜这32支球队那个是冠军。并且猜测错误付出代价。每猜错一次给一块钱，告诉我是否猜对了，那么我需要掏多少钱才能知道谁是冠军？ （前提是：不知道任意球队的信息、历史比赛记录、实力等）

![image-20210531100323227](images/机器学习(未全)/image-20210531100323227.png)

**为了使代价最小，可以使用二分法猜测：**

我可以把球编上号，从1到32，然后提问：冠 军在1-16号吗？依次询问，只需要五次，就可以知道结果。

![image-20210531100334495](images/机器学习(未全)/image-20210531100334495.png)

我们来看这个式子：

- 32支球队，log32=5**比特**
- 64支球队，log64=6**比特**

![image-20210531100354466](images/机器学习(未全)/image-20210531100354466.png)

**香农指出，它的准确信息量应该是，p为每个球队获胜的概率（假设概率相等，都为1/32），我们不用钱去衡量这个代价了，香浓指出用比特**：

```python
H = -(p1logp1 + p2logp2 + ... + p32log32) = - log32
```

#### 2.2.1 信息熵的定义

- H的专业术语称之为信息熵，单位为比特。

![image-20210531100408710](images/机器学习(未全)/image-20210531100408710.png)

“谁是世界杯冠军”的信息量应该比5比特少，特点（重要）：

- 当这32支球队夺冠的几率相同时，对应的信息熵等于5比特
- 只要概率发生任意变化，信息熵都比5比特大

#### 2.2.2 总结（重要）

- 信息和消除不确定性是相联系的

当我们得到的额外信息（球队历史比赛情况等等）越多的话，那么我们猜测的代价越小（猜测的不确定性减小）

#### 问题： 回到我们前面的贷款案例，怎么去划分？可以利用当得知某个特征（比如是否有房子）之后，我们能够减少的不确定性大小。越大我们可以认为这个特征很重要。那怎么去衡量减少的不确定性大小呢？

### 2.3 决策树的划分依据之一------信息增益

#### 2.3.1 定义与公式

特征A对训练数据集D的信息增益g(D,A),定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差，即公式为：

![image-20210531100425882](images/机器学习(未全)/image-20210531100425882.png)

公式的详细解释：

![image-20210531100443606](images/机器学习(未全)/image-20210531100443606.png)

> 注：信息增益表示得知特征X的信息而息的不确定性减少的程度使得类Y的信息熵减少的程度

#### 2.3.2 贷款特征重要计算

![image-20210531100500918](images/机器学习(未全)/image-20210531100500918.png)

- 我们以年龄特征来计算：

```python
1、g(D, 年龄) = H(D) -H(D|年龄) = 0.971-[5/15H(青年)+5/15H(中年)+5/15H(老年]

2、H(D) = -(6/15log(6/15)+9/15log(9/15))=0.971

3、H(青年) = -(3/5log(3/5) +2/5log(2/5))
H(中年)=-(3/5log(3/5) +2/5log(2/5))
H(老年)=-(4/5og(4/5)+1/5log(1/5))
```

我们以A1、A2、A3、A4代表年龄、有工作、有自己的房子和贷款情况。最终计算的结果g(D, A1) = 0.313, g(D, A2) = 0.324, g(D, A3) = 0.420,g(D, A4) = 0.363。所以我们选择A3 作为划分的第一个特征。这样我们就可以一棵树慢慢建立

### 2.4 决策树的三种算法实现

当然决策树的原理不止信息增益这一种，还有其他方法。但是原理都类似，我们就不去举例计算。

- ID3
  - 信息增益 最大的准则
- C4.5
  - 信息增益比 最大的准则
- CART
  - 分类树: 基尼系数 最小的准则 在sklearn中可以选择划分的默认原则
  - 优势：划分更加细致（从后面例子的树显示来理解）

### 2.5 决策树API

- class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)
  - 决策树分类器
  - criterion:默认是’gini’系数，也可以选择信息增益的熵’entropy’
  - max_depth:树的深度大小
  - random_state:随机数种子
- 其中会有些超参数：max_depth:树的深度大小
  - 其它超参数我们会结合随机森林讲解

## 3、案例：泰坦尼克号乘客生存预测

- 泰坦尼克号数据

在泰坦尼克号和titanic2数据帧描述泰坦尼克号上的个别乘客的生存状态。这里使用的数据集是由各种研究人员开始的。其中包括许多研究人员创建的旅客名单，由Michael A. Findlay编辑。我们提取的数据集中的特征是票的类别，存活，乘坐班，年龄，登陆，home.dest，房间，票，船和性别。

**1、乘坐班是指乘客班（1，2，3），是社会经济阶层的代表。**

**2、其中age数据存在缺失。**

> 数据：http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt

![image-20210531100533540](images/机器学习(未全)/image-20210531100533540.png)

### 3.1 分析

- 选择我们认为重要的几个特征 ['pclass', 'age', 'sex']
- 填充缺失值
- 特征中出现类别符号，需要进行one-hot编码处理(DictVectorizer)
  - x.to_dict(orient="records") 需要将数组特征转换成字典数据
- 数据集划分
- 决策树分类预测

### 3.2 代码

```python
def decisioncls():
    """
    决策树进行乘客生存预测
    :return:
    """
    # 1、获取数据
    titan = pd.read_csv("http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt")

    # 2、数据的处理
    x = titan[['pclass', 'age', 'sex']]

    y = titan['survived']

    # print(x , y)
    # 缺失值需要处理，将特征当中有类别的这些特征进行字典特征抽取
    x['age'].fillna(x['age'].mean(), inplace=True)

    # 对于x转换成字典数据x.to_dict(orient="records")
    # [{"pclass": "1st", "age": 29.00, "sex": "female"}, {}]

    dict = DictVectorizer(sparse=False)

    x = dict.fit_transform(x.to_dict(orient="records"))

    print(dict.get_feature_names())
    print(x)

    # 分割训练集合测试集
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)

    # 进行决策树的建立和预测
    dc = DecisionTreeClassifier(max_depth=5)

    dc.fit(x_train, y_train)

    print("预测的准确率为：", dc.score(x_test, y_test))

    return None
```

由于决策树类似一个树的结构，我们可以保存到本地显示

### 3.3 保存树的结构到dot文件(决策树可视化)

- 1、sklearn.tree.export_graphviz() 该函数能够导出DOT格式
  - tree.export_graphviz(estimator,out_file='tree.dot’,feature_names=[‘’,’’])
- 2、工具:(能够将dot文件转换为pdf、png)
  - 安装graphviz
  - ubuntu:sudo apt-get install graphviz Mac:brew install graphviz
- 3、运行命令
  - 然后我们运行这个命令
  - dot -Tpng tree.dot -o tree.png

```python
export_graphviz(dc, out_file="./tree.dot", feature_names=['age', 'pclass=1st', 'pclass=2nd', 'pclass=3rd', '女性', '男性'])
```

## 4、 决策树总结

- 优点：
  - 简单的理解和解释，树木可视化。
- 缺点：
  - **决策树学习者可以创建不能很好地推广数据的过于复杂的树，这被称为过拟合。**
- 改进：
  - 减枝cart算法(决策树API当中已经实现，随机森林参数调优有相关介绍)
  - **随机森林**

**注：企业重要决策，由于决策树很好的分析能力，在决策过程应用较多， 可以选择特征**

# 集成学习方法之随机森林

## 学习目标

- 目标
  - 说名随机森林每棵决策树的建立过程
  - 知道为什么需要随机有放回(Bootstrap)的抽样
  - 说明随机森林的超参数
- 应用
  - 泰坦尼克号乘客生存预测

## 1、 什么是集成学习方法

集成学习通过建立几个模型组合的来解决单一预测问题。它的工作原理是**生成多个分类器/模型**，各自独立地学习和作出预测。**这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测。**

## 2、 什么是随机森林

在机器学习中，**随机森林是一个包含多个决策树的分类器**，并且其输出的类别是由个别树输出的类别的众数而定。

![image-20210531113012101](images/机器学习(未全)/image-20210531113012101.png)

例如, 如果你训练了5个树, 其中有4个树的结果是True, 1个数的结果是False, 那么最终投票结果就是True

![image-20210531114340261](images/机器学习(未全)/image-20210531114340261.png)

## 3、 随机森林原理过程

学习算法根据下列算法而建造每棵树：

- 用N来表示训练用例（样本）的个数，M表示特征数目。
  - 1、一次随机选出一个样本，重复N次， （有可能出现重复的样本）
  - 2、随机去选出m个特征, m <<M，建立决策树
- 采取bootstrap抽样

### 3.1 为什么采用BootStrap抽样

- 为什么要随机抽样训练集？　　
  - 如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的
- 为什么要有放回地抽样？
  - 如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是“有偏的”，都是绝对“片面的”（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决。

### 3.2 API

- class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2)
  - 随机森林分类器
  - n_estimators：integer，optional（default = 10）森林里的树木数量120,200,300,500,800,1200
  - criteria：string，可选（default =“gini”）分割特征的测量方法
  - max_depth：integer或None，可选（默认=无）树的最大深度 5,8,15,25,30
  - max_features="auto”,每个决策树的最大特征数量
    - If "auto", then `max_features=sqrt(n_features)`.
    - If "sqrt", then `max_features=sqrt(n_features)` (same as "auto").
    - If "log2", then `max_features=log2(n_features)`.
    - If None, then `max_features=n_features`.
  - bootstrap：boolean，optional（default = True）是否在构建树时使用放回抽样
  - min_samples_split:节点划分最少样本数
  - min_samples_leaf:叶子节点的最小样本数

- 超参数：n_estimator, max_depth, min_samples_split,min_samples_leaf

### 3.3 代码

```python
# 随机森林去进行预测
rf = RandomForestClassifier()

param = {"n_estimators": [120,200,300,500,800,1200], "max_depth": [5, 8, 15, 25, 30]}

# 超参数调优
gc = GridSearchCV(rf, param_grid=param, cv=2)

gc.fit(x_train, y_train)

print("随机森林预测的准确率为：", gc.score(x_test, y_test))
```

## 4、总结

- 在当前所有算法中，具有极好的准确率
- 能够有效地运行在大数据集上，处理具有高维特征的输入样本，而且不需要降维
- 能够评估各个特征在分类问题上的重要性